Embedding Extraction App
========================

This workflow retrieves all images from ApertureDB that has not been
analyzed before, and runs them through a
[CLIP (Contrastive Languageâ€“Image Pre-training)](https://openai.com/index/clip/)
model to extract an embedding for each image.

Each image is updated with a flag (`wf_embeddings_clip`), and the embedding extracted
is then inserted to ApertureDB and connected to the image as a `Descriptor` object.
This allows `knn` queries to find similar images, either by generating
a query embedding from another image or from a text prompt.

The workflow will run on a infinite loop.

Parameters
----------

`MODEL_NAME`: Specifies the model to be used.
Available options are: ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']. Default is `ViT-B/16`.

`NUMTHREAD`: Specifies the number of threads that will be running simultaneously,
retrieving and computing embeddings in parallel. Default is `4`.

`CLEAN`: Boolean flag specifying whether all objects generated by previous runs
of this workflow will be cleaned before starting the retrieval and insertion.
Default is `false`.

`RUN_ONCE`: Boolean flag specifying whether the workflow will run on an infinite
loop, or if it will exit upon completion, without watching for new images.
Default is `false`.

Usage
-----

```
docker run \
           --net=host \
           -e RUN_NAME=my_testing_run \
           -e DB_HOST=workflowstesting.gcp.cloud.aperturedata.dev \
           -e DB_PASS="password" \
           -e MODEL_NAME="ViT-B/32" \
           aperturedata/workflows-embeddings-extraction
```
