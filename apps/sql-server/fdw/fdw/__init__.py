from .common import get_log_level, compact_pretty_json, get_command_body, PathKey
from .table import TableOptions
from .column import ColumnOptions
from multicorn import ForeignDataWrapper, Qual
import numpy as np
import atexit
import json
import os
import logging
from datetime import datetime
import sys
from itertools import zip_longest
from typing import Optional, Set, Tuple, Generator, List, Dict, Any, Iterable, Callable

import pydantic
import sys
import importlib.util
from .aperturedb import execute_query


# Configure logging
log_level = get_log_level()
handler = logging.FileHandler("/tmp/fdw.log", delay=False)
handler.setFormatter(logging.Formatter(
    "%(asctime)s %(levelname)s %(message)s"))

logging.basicConfig(level=log_level, force=True)
logger = logging.getLogger(__name__)
logger.setLevel(log_level)
logger.addHandler(handler)
logger.propagate = False


def flush_logs():
    for h in logger.handlers:
        try:
            h.flush()
        except Exception:
            pass


atexit.register(flush_logs)

# Queries are processed in batches, but the client doesn't know because result rows are yielded one by one.
BATCH_SIZE = 100
BATCH_SIZE_WITH_BLOBS = 10

# Estimated size of a column in bytes.
TYPE_SIZE_ESTIMATE = {
    "boolean": 1,
    "number": 4,  # float32
    "datetime": 8,  # int64 for timestamp
    "string": 40,  # average string length
    "json": 100,  # average JSON size
    "blob": 1000,  # average blob size
    "uniqueid": 16,  # UUID size
    None: 10,  # default size, for columns that are not listable
}


class FDW(ForeignDataWrapper):
    """
    A Foreign Data Wrapper (FDW) for ApertureDB.
    This class allows PostgreSQL to interact with ApertureDB as if it were a foreign data source.
    Note that this class is instantiated once for each foreign table.
    The class method `import_schema` says what tables and columns to create in PostgreSQL.
    It also passes options for each table and column that are passed into `__init__`.
    """

    @classmethod
    def import_schema(cls, schema, srv_options, options, restriction_type, restricts):
        """
        Import the schema from ApertureDB and return a list of TableDefinitions.
        This method is called when the foreign data wrapper is created.
        The result of this is to create the foreign tables in PostgreSQL.

        Note that we cannot add comments, foreign keys, or other constraints here.

        This method is called once per schema.
        """
        try:
            # Put these here for better error handling
            from .system import system_schema
            from .entity import entity_schema
            from .connection import connection_schema
            from .descriptor import descriptor_schema

            logger.info(f"Importing schema {schema} with options: {options}")
            if schema == "system":
                return system_schema()
            elif schema == "entity":
                return entity_schema()
            elif schema == "connection":
                return connection_schema()
            elif schema == "descriptor":
                return descriptor_schema()
            else:
                raise ValueError(f"Unknown schema: {schema}")
        except:
            logger.exception(
                f"Error importing schema {schema}: {sys.exc_info()[1]}")
            flush_logs()
            raise
        logger.info(f"Schema {schema} imported successfully")

    def __init__(self, fdw_options, fdw_columns):
        """
        Initialize the FDW with the given options and columns,
        which are generated by the import_schema method.

        Args:
            fdw_options (dict): Options for the foreign table.
            fdw_columns (dict): Columns for the foreign table.
        """
        super().__init__(fdw_options, fdw_columns)

        self._options = TableOptions.from_string(fdw_options)
        self._columns = {
            name: ColumnOptions.from_string(col.options)
            for name, col in fdw_columns.items()}
        logger.info(f"FDW {self._options.table_name} initialized")

    def execute(self, quals: List[Qual], columns: Set[str]) -> Iterable[dict]:
        """ Execute the FDW query with the given quals and columns.

        Args:
            quals (list): List of conditions to filter the results.
                Note that filtering is optional because PostgreSQL will also filter the results.
            columns (set): List of columns to return in the results.
        """

        start_time = datetime.now()
        logger.info(
            f"Executing FDW {self._options.table_name} with quals: {quals} and columns: {columns}")

        self._check_quals(quals, columns)

        query, get_result_objects = self._get_query(quals, columns)

        query_blobs = self._get_query_blobs(quals, columns)

        n_results = 0
        total_elapsed_time = 0
        exhausted = False
        n_queries = 0
        try:
            while query:
                n_queries += 1
                gen = self._get_query_results(
                    query, query_blobs, get_result_objects)
                try:
                    while True:
                        row, blob = next(gen)
                        result = self._post_process_row(
                            quals, columns, row, blob)

                        logger.debug(
                            f"Yielding row: {json.dumps({k: v[:10] if isinstance(v, str) else len(v) if isinstance(v, (bytes, list)) else v for k, v in row.items()})} blob {len(blob) if blob else None}"
                        )
                        n_results += 1
                        if n_results % 1000 == 0:
                            logger.info(
                                f"Yielded {n_results} results so far for FDW {self._options.table_name}")
                        yield result
                except StopIteration as e:
                    response, elapsed_time = e.value  # return value from _get_query_results
                    total_elapsed_time += elapsed_time.total_seconds()

                query = self._get_next_query(query, response)
            exhausted = True
        finally:
            elapsed_time = datetime.now() - start_time
            logger.info(
                f"Executed FDW {self._options.table_name} with {n_results} results and {n_queries} queries in {total_elapsed_time:.2f} seconds in ADB, {elapsed_time.total_seconds():.2f} seconds in execute, {'exhausted' if exhausted else 'not exhausted'}.")

    def _check_quals(self, quals: List[Qual], columns: Set[str]) -> None:
        """
        Check the quals to ensure they are valid.
        """
        for col in columns:
            col_type = self._columns[col].type
            if not self._columns[col].listable:  # special column; apply checks
                clauses = [qual for qual in quals if qual.field_name == col]

                if len(clauses) > 1:
                    raise ValueError(
                        f"Multiple WHERE clauses for {col} are not allowed, got {len(clauses)} clauses.")
                for clause in clauses:
                    if col_type == "boolean":
                        if clause.operator not in ["=", "<>", "IS", "IS NOT"]:
                            raise ValueError(
                                f"WHERE clauses for boolean column {col} can only use operators '=', '<>', 'IS', or 'IS NOT', got {clause.operator}")
                    else:
                        if clause.operator not in ["="]:
                            raise ValueError(
                                f"WHERE clauses for non-boolean column {col} can only use operators '=', got {clause.operator}")

    def _get_query(self,
                   quals: List[Qual],
                   columns: Set[str],
                   ) -> List[dict]:
        """
        Construct the query to execute against ApertureDB.
        This is used to build the query based on the columns and options.
        """
        listable_columns = {
            col for col in columns if self._columns[col].listable}

        modifying_columns = {
            col for col in columns if self._columns[col].modify_command_body is not None}

        command_body = {}

        # apply constraints
        constraints = self._generate_constraints(quals, columns)
        if constraints:
            command_body["constraints"] = constraints

        # Apply column modifications
        for qual in quals:
            if qual.field_name in modifying_columns:

                n_clauses = len(
                    [qual2 for qual2 in quals if qual2.field_name == qual.field_name])
                if n_clauses > 1:
                    raise ValueError(
                        f"Multiple WHERE clauses for {qual.field_name} are not allowed, got {n_clauses} clauses.")

                value = self._convert_qual_value(qual)
                column_options = self._columns[qual.field_name]
                column_options.modify_command_body(
                    command_body=command_body, value=value)

        # Check whether anyone has added the blobs parameter
        blobs = command_body.get("blobs", False)
        batch_size = BATCH_SIZE_WITH_BLOBS if blobs else BATCH_SIZE
        command_body["batch"] = {
            "batch_id": 0,
            "batch_size": batch_size
        }

        # In SQL it is legal to query a table with no result columns.
        # This will (by design) cause an error in ApertureDB.
        if not blobs and not listable_columns:
            listable_columns = {"_uniqueid"}

        if listable_columns:
            command_body["results"] = {"list": list(listable_columns)}

        query = [{self._options.command: command_body}]

        # Apply table modification, e.g. with_class, set
        if self._options.modify_query:
            get_result_objects = self._options.modify_query(query=query)
        else:
            get_result_objects = None

        logger.debug(
            f"Constructed query: {compact_pretty_json(query, indent=2)}")

        return query, get_result_objects

    def _generate_constraints(self, quals: List[Qual], columns: Set[str]) -> dict:
        """
        Generate constraints for the command body based on the provided qualifications and columns.
        """
        constraints = {}
        listable_columns = {
            col for col in columns if self._columns[col].listable}
        for qual in quals:
            if qual.field_name in listable_columns:
                constraint = self._qual_to_constraint(qual)
                if constraint is not None:
                    logger.debug(
                        f"Applying constraint {constraint} for qual {qual} on column {qual.field_name} in {self._options.table_name}")
                    if qual.field_name not in constraints:
                        constraints[qual.field_name] = []
                    constraints[qual.field_name].extend(constraint)
        return constraints

    def _qual_to_constraint(self, qual: Qual) -> Optional[List[Any]]:
        """
        Generate constraint for the command body based on the provided qualification.
        """
        exclude_all = ["in", []]  # Exclude all rows

        col_type = self._columns[qual.field_name].type
        value = self._convert_qual_value(qual, use_operator=False)
        if col_type == "number":
            assert value is None or isinstance(
                value, (int, float)) or (isinstance(value, list) and all(isinstance(x, (int, float)) for x in value)), f"Qual {qual} for column {qual.field_name} must be a number for operator {qual.operator}, found {value} is {type(value)} (was {type(qual.value)})."
        logger.debug(
            f"Applying constraint for qual {qual} with field name {qual.field_name}, operator {qual.operator}, value {value} and column type {col_type}")
        if qual.operator == "=":
            if isinstance(value, list):
                return ["in", value]
            elif col_type != 'uniqueid' or value is not None:
                return ["==", value]
        elif qual.operator == "<>":
            # SQL NULL semantics will not include results where a column is NULL, but ApertureDB will.
            if value is None:
                # Surpisingly, this arises from a clause like "b IS NOT NULL", and not from "b <> NULL".
                if col_type != 'uniqueid':
                    return ["!=", None]  # Exclude all rows with NULL
            elif isinstance(value, list):
                return ["not in", value]
            elif col_type == 'boolean':
                return ["==", not value]
            else:
                # PARTIAL: Will include unset values, but will be post-filtered by PostgreSQL.
                return ["!=", value]
        elif qual.operator == "IS":
            assert col_type == 'boolean' or value is None, f"Qual {qual} for column {qual.field_name} must be boolean or None for IS operator."
            if col_type != 'uniqueid':
                return ["==", value]
        elif qual.operator == "IS NOT":
            assert col_type == 'boolean' or value is None, f"Qual {qual} for column {qual.field_name} must be boolean or None for IS NOT operator."
            # This will include null values (if value is not None)
            if col_type != 'uniqueid':
                return ["!=", value]
        elif qual.operator == "IN" or qual.operator == ("=", True):
            assert isinstance(
                value, list), f"Qual {qual} for column {qual.field_name} must be a list for IN operator."
            if None in value:  # SQL NULL handling rules
                value_without_none = [v for v in value if v is not None]
                return ["in", value_without_none]
            else:
                return ["in", value]
        elif qual.operator == "NOT IN" or qual.operator == ("<>", False):
            assert isinstance(
                value, list), f"Qual {qual} for column {qual.field_name} must be a list for NOT IN operator."
            if None in value:
                return exclude_all  # exclude all rows; SQL NULL handling rules
            else:
                # PARTIAL: This should not include null values, but ApertureDB will include them. Safe because of post-filter.
                return ["not in", value]
        elif col_type == "boolean":
            # Standard SQL does not support < or > for boolean columns, but this is a PostgreSQL-specific extension.
            # FALSE < TRUE, and comparison with NULL is always UNKNOWN which is treated as FALSE.
            if qual.operator == "<":
                if value is True:
                    return ["==", False]
                elif value is False:
                    return exclude_all
            elif qual.operator == "<=":
                if value is False:
                    return ["==", False]
                elif value is True:
                    return ["!=", None]
            elif qual.operator == ">":
                if value is False:
                    return ["==", True]
                elif value is True:
                    return exclude_all
            elif qual.operator == ">=":
                if value is True:
                    return ["==", True]
                elif value is False:
                    return ["in", [False, True]]
            # everything else is a no-op for boolean columns
        elif col_type in {"datetime", "number", "string"} and qual.operator in {"<", "<=", ">", ">="}:
            return [qual.operator, value]
        # Skip this qual; PostgreSQL will filter it out later if it's important.
        logger.debug(
            f"Ignoring qual {qual}, col_type {col_type}, value {value}")
        return None

    def _convert_qual_value(self, qual: Qual, use_operator=True) -> Any:
        """
        Convert qual value into an internal value, depending on type and operator.
        """
        if isinstance(qual.value, (list, tuple)):
            return [self._convert_qual_value(Qual(qual.field_name, qual.operator, x), use_operator=use_operator) for x in qual.value]

        col_type = self._columns[qual.field_name].type
        if col_type == "boolean":
            value = True if qual.value == 't' else False if qual.value == 'f' else None
            if qual.operator in ["<>", "IS NOT"] and use_operator and value in [True, False]:
                value = not value
        elif col_type == "json":
            try:
                value = json.loads(qual.value)
            except json.JSONDecodeError as e:
                raise ValueError(
                    f"Invalid JSON in {qual.field_name} clause: {e}")
        elif col_type == "datetime":
            # convert datetime datetime objects to the internal format
            if isinstance(qual.value, datetime):
                value = {"_date": qual.value.isoformat()}
            else:
                raise ValueError(
                    f"Invalid datetime value for {qual.field_name}: {qual.value}. Expected a datetime object.")
        elif col_type == "number":
            value = float(qual.value) if qual.value is not None else None
        else:
            # TODO: Might be more conversions needed here
            value = qual.value

        return value

    def _get_query_blobs(self, quals: List[Qual], columns: Set[str]) -> List[bytes]:
        query_blob_quals = [qual for qual in quals
                            if self._columns[qual.field_name].query_blobs is not None]

        if query_blob_quals:
            if len(query_blob_quals) > 1:
                raise ValueError(
                    f"Multiple query blobs requested: {query_blob_quals}. Only one query blob is allowed per query.")

            qual = query_blob_quals[0]
            value = self._convert_qual_value(qual)
            return self._columns[qual.field_name].query_blobs(value=value)

        return []

    def _get_query_results(self,
                           query: List[dict],
                           query_blobs: List[bytes],
                           get_result_objects: Optional[Callable],
                           ) -> Generator[Tuple[dict, Optional[bytes]], None, List[dict]]:
        start_time = datetime.now()
        status, results, response_blobs = execute_query(query, query_blobs)
        elapsed_time = datetime.now() - start_time

        if not results or \
            len(results) != len(query) or \
                not isinstance(results, list) or \
            status != 0:
            logger.warning(
                f"Query Error:{query} -> {status} {results}")
            raise ValueError(
                f"Query Error: {query}. Please check the class and columns. Results: {status} {results}")

        if get_result_objects:
            # If a special function is provided, apply it to the results.
            result_objects = get_result_objects(results)
        else:
            result_objects = results[-1].get(
                self._options.command, {}).get(self._options.result_field, [])

        if not response_blobs:
            for row in result_objects:
                yield row, None
        else:
            for row, blob in zip_longest(result_objects, response_blobs):
                yield row or {}, blob

        return results, elapsed_time

    def _post_process_row(self,
                          quals: List[Qual], columns: Set[str],
                          row: dict, blob: Optional[bytes]) -> dict:
        """
        Apply post-processing steps to the row before yielding it.

        This includes converting types from AQL to SQL,
        adding non-list columns, and any column post-processing.
        """
        row = row.copy()  # Avoid modifying the original row

        for qual in quals:
            if self._columns[qual.field_name].post_process_results is not None:
                value = self._convert_qual_value(qual)
                self._columns[qual.field_name].post_process_results(
                    row=row, value=value, blob=blob)

        # Normalize the row to ensure it has the correct types for PostgreSQL
        row = self._normalize_row(columns, row)

        row = self._add_non_list_columns(quals, columns, row)

        return row

    def _normalize_row(self, columns, row: dict) -> dict:
        """
        Normalize a row to ensure it has the correct types for PostgreSQL.
        This is used to convert ApertureDB types to PostgreSQL types.
        """
        result = {}
        for col in columns:
            if col not in row:
                continue
            value = self._convert_adb_value(row[col], col)
            result[col] = value
        return result

    def _convert_adb_value(self, value, col: str) -> Any:
        """
        Convert an ApertureDB value to a PostgreSQL-compatible value.
        """
        col_type = self._columns[col].type
        if col_type == "datetime":
            value = value["_date"] if value else None
        elif col_type == "json":
            value = json.dumps(value)
        elif col_type == "blob":
            value = value
        # elif col_type == "boolean":
        #     value = 't' if value else 'f'
        else:
            value = value
        return value

    def _add_non_list_columns(self, quals: List[Qual], columns: Set[str],  row: dict) -> dict:
        """
        Add non-list columns to the row based on the quals.
        This is used to ensure that all requested columns are present in the result.

        This is necessary because PostgreSQL will not return rows that don't meet the quals, and it doesn't know that we're
        using special columns to do magic.

        So we just copy the qual constraints into the row.
        """
        row = row.copy()  # Avoid modifying the original row
        for qual in quals:
            if not self._columns[qual.field_name].listable:
                assert qual.field_name not in row, f"Column {qual.field_name} should not be in the row. It is a non-list column."
                # This double-conversion is necessary because of negative qual operators like IS NOT and <>.
                value = self._convert_qual_value(qual)
                value = self._convert_adb_value(value, qual.field_name)
                row[qual.field_name] = value

        return row

    def _get_next_query(self, query: List[dict], response: List[dict]) -> Optional[List[dict]]:
        """
        Get the next query to execute based on the response from the previous query.
        This is used to handle batching.
        """
        if not response or len(response) != len(query) or not isinstance(response, list):
            logger.warning(
                f"No results found for query: {query} -> {response}")
            return None

        command_body = get_command_body(query[-1])
        response_body = get_command_body(response[-1])

        if "batch" in response_body:
            batch_id = response_body["batch"]["batch_id"]
            total_elements = response_body["batch"]["total_elements"]
            end = response_body["batch"]["end"]

            if end >= total_elements:  # No more batches to process
                return None

            next_query = query.copy()
            next_command_body = get_command_body(next_query[-1])
            next_command_body["batch"]["batch_id"] += 1
            return next_query
        elif "limit" in command_body:
            # fallback mode for commands that don't support batching
            limit = command_body["limit"]
            returned = response_body.get("returned", 0)
            if returned == 0:
                logger.info(
                    f"No results returned for query: {query} -> {response[:10]}")
                return None
            offset = command_body.get("offset", 0)

            next_query = query.copy()
            next_command_body = get_command_body(next_query[-1])
            next_command_body["offset"] = offset + limit
            return next_query
        else:
            # Some commands (like FindConnection) might not handle batching, so we assume all results are returned at once.
            # https://github.com/aperture-data/athena/issues/1727
            logger.info(
                f"Single batch found for query: {query} -> {response[:10]}")
            return None

    def explain(self, quals: List[Qual], columns: Set[str], sortkeys=None, verbose=False) -> Iterable[str]:
        """
        Generate an EXPLAIN statement for the FDW query.
        This is used to provide information about how the query will be executed.
        """
        logger.info(
            f"Explaining FDW {self._options.table_name} with quals: {quals} and columns: {columns}")
        self._check_quals(quals, columns)

        result = {}

        result["table_name"] = self._options.table_name
        result["quals"] = [[qual.field_name, qual.operator, qual.value]
                           for qual in quals]
        result["columns"] = list(columns)
        result["aql"] = self._get_query(quals, columns)[0]
        # This part isn't verbose, but can be much slower
        if verbose:
            query_blobs = self._get_query_blobs(quals, columns)
            if query_blobs:
                result["query_blob_lengths"] = [
                    len(blob) for blob in query_blobs]

        return [compact_pretty_json(result) if verbose else json.dumps(result)]

    def get_rel_size(self, quals: List[Qual], columns: Set[str]) -> Tuple[int, int]:
        """
        https://github.com/pgsql-io/multicorn2/blob/7ab7f0bcfe6052ebb318ed982df8dfd78ce5ee6a/python/multicorn/__init__.py#L172
        https://www.postgresql.org/docs/current/fdw-callbacks.html#FDW-CALLBACKS-SCAN
        """
        logger.info(
            f"Estimating relation size for FDW {self._options.table_name} with quals: {quals} and columns: {columns}")
        query = self._get_query(quals, columns)[0]
        command_body = get_command_body(query[-1])
        blobs = command_body.get("blobs", False)

        n_rows = self._options.count
        for col, constraints in command_body.get("constraints", {}).items():
            col_type = self._columns[col].type
            if col == "_uniqueid":
                if constraints[0] == "==":
                    n_rows = 1  # Unique ID means only one row can match
                elif constraints[0] == "in":
                    # Number of unique IDs in the list
                    n_rows = len(constraints[1])
                # negative constraints don't significantly affect the number of rows
            elif col_type == "boolean":
                n_rows //= 2  # Boolean constraints halve the number of rows
            elif col_type in {"number", "datetime", "uniqueid", "string"}:
                if constraints[0] in {"==", "in"}:
                    n_rows //= 10  # Equality constraints reduce the number of rows significantly
                elif constraints[0] in {"<", "<=", ">", ">="}:
                    n_rows //= 2  # Range constraints halve the number of rows

        n_rows = max(n_rows, 1)  # Ensure at least one row

        width = sum(
            TYPE_SIZE_ESTIMATE[self._columns[col].type] for col in columns if blobs or self._columns[col].type != "blob")

        logger.info(
            f"Estimated size for FDW {self._options.table_name}: {n_rows} rows, width {width} bytes per row")
        inflation = 10  # inflate the size as a crude way to account for per-query overhead; see comment in get_path_keys
        return (n_rows * inflation, width)

    def get_path_keys(self) -> List[Tuple[List[str], int]]:
        """
        https://github.com/pgsql-io/multicorn2/blob/7ab7f0bcfe6052ebb318ed982df8dfd78ce5ee6a/python/multicorn/__init__.py#L215

        This method is roughy intended to return the indexed keys for the foreign table.
        It is used to optimize the query planning by PostgreSQL.
        """
        keys: List[PathKey] = self._options.path_keys

        # Unfortunately, full implementation of this feature can result in Postgres doing joins one row at a time,
        # as it does not perceive any static per-query overhead.
        # This is not ideal, so we do not implement it fully.
        # In the long term, we might want to extend Multicorn to support static costs for queries.

        filter_uniqueids = True
        if filter_uniqueids:
            keys = [key for key in keys if not any(
                self._columns[col].type == "uniqueid" for col in key.columns)]

        logger.info(
            f"Getting path keys for FDW {self._options.table_name} -> {keys}")
        # Convert PathKey to the expected format
        result = [
            (key.columns, key.expected_rows) for key in keys if key.expected_rows > 0
        ]
        return result


logger.info("FDW class defined successfully")
