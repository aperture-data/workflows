from .common import get_pool, get_log_level, compact_pretty_json
from .table import TableOptions
from .column import ColumnOptions
from multicorn import ForeignDataWrapper, Qual
import numpy as np
import atexit
import json
import os
import logging
from datetime import datetime
import sys
from itertools import zip_longest
from typing import Optional, Set, Tuple, Generator, List, Dict, Any, Iterable

import pydantic
import sys
import importlib.util


# Configure logging
log_level = get_log_level()
handler = logging.FileHandler("/tmp/fdw.log", delay=False)
handler.setFormatter(logging.Formatter(
    "%(asctime)s %(levelname)s %(message)s"))

logging.basicConfig(level=log_level, force=True)
logger = logging.getLogger(__name__)
logger.setLevel(log_level)
logger.addHandler(handler)
logger.propagate = False


def flush_logs():
    for h in logger.handlers:
        try:
            h.flush()
        except Exception:
            pass


atexit.register(flush_logs)

# Queries are processed in batches, but the client doesn't know because result rows are yielded one by one.
BATCH_SIZE = 100
BATCH_SIZE_WITH_BLOBS = 10


class FDW(ForeignDataWrapper):
    """
    A Foreign Data Wrapper (FDW) for ApertureDB.
    This class allows PostgreSQL to interact with ApertureDB as if it were a foreign data source.
    Note that this class is instantiated once for each foreign table.
    The class method `import_schema` says what tables and columns to create in PostgreSQL.
    It also passes options for each table and column that are passed into `__init__`.
    """

    @classmethod
    def import_schema(cls, schema, srv_options, options, restriction_type, restricts):
        """
        Import the schema from ApertureDB and return a list of TableDefinitions.
        This method is called when the foreign data wrapper is created.
        The result of this is to create the foreign tables in PostgreSQL.

        Note that we cannot add comments, foreign keys, or other constraints here.

        This method is called once per schema.
        """
        try:
            # Put these here for better error handling
            from .system import system_schema
            from .entity import entity_schema
            from .connection import connection_schema
            from .descriptor import descriptor_schema

            logger.info(f"Importing schema {schema} with options: {options}")
            if schema == "system":
                return system_schema()
            elif schema == "entity":
                return entity_schema()
            elif schema == "connection":
                return connection_schema()
            elif schema == "descriptor":
                return descriptor_schema()
            else:
                raise ValueError(f"Unknown schema: {schema}")
        except:
            logger.exception(
                f"Error importing schema {schema}: {sys.exc_info()[1]}")
            flush_logs()
            raise
        logger.info(f"Schema {schema} imported successfully")

    def __init__(self, fdw_options, fdw_columns):
        """
        Initialize the FDW with the given options and columns,
        which are generated by the import_schema method.

        Args:
            fdw_options (dict): Options for the foreign table.
            fdw_columns (dict): Columns for the foreign table.
        """
        super().__init__(fdw_options, fdw_columns)

        self._options = TableOptions.from_string(fdw_options)
        self._columns = {
            name: ColumnOptions.from_string(col.options)
            for name, col in fdw_columns.items()}
        logger.info("FDW initialized with options: %s", fdw_options)

    def execute(self, quals: List[Qual], columns: Set[str]) -> Generator[dict, None, None]:
        """ Execute the FDW query with the given quals and columns.

        Args:
            quals (list): List of conditions to filter the results.
                Note that filtering is optional because PostgreSQL will also filter the results.
            columns (set): List of columns to return in the results.
        """

        start_time = datetime.now()
        logger.info(
            f"Executing FDW {self._options.table_name} with quals: {quals} and columns: {columns}")

        self._check_quals(quals, columns)

        query = self._get_query(quals, columns)

        query_blobs = self._get_query_blobs(quals, columns)

        n_results = 0
        total_elapsed_time = 0
        exhausted = False
        n_queries = 0
        try:
            while query:
                n_queries += 1
                gen = self._get_query_results(query, query_blobs)
                try:
                    while True:
                        row, blob = next(gen)
                        result = self._post_process_row(
                            quals, columns, row, blob)

                        logger.debug(
                            f"Yielding row: {json.dumps({k: v[:10] if isinstance(v, str) else len(v) if isinstance(v, (bytes, list)) else v for k, v in row.items()}, indent=2)} blob {len(blob) if blob else None}"
                        )
                        n_results += 1
                        if n_results % 1000 == 0:
                            logger.info(
                                f"Yielded {n_results} results so far for FDW {self._options.table_name}")
                        yield result
                except StopIteration as e:
                    response, elapsed_time = e.value  # return value from _get_query_results
                    total_elapsed_time += elapsed_time.total_seconds()

                query = self._get_next_query(query, response)
            exhausted = True
        finally:
            elapsed_time = datetime.now() - start_time
            logger.info(
                f"Executed FDW {self._options.table_name} with {n_results} results and {n_queries} queries in {total_elapsed_time:.2f} seconds in ADB, {elapsed_time.total_seconds():.2f} seconds in execute, {'exhausted' if exhausted else 'not exhausted'}.")

    def _check_quals(self, quals: List[Qual], columns: Set[str]) -> None:
        """
        Check the quals to ensure they are valid.
        """
        for col in columns:
            col_type = self._columns[col].type
            if not self._columns[col].listable:  # special column; apply checks
                clauses = [qual for qual in quals if qual.field_name == col]

                if len(clauses) > 1:
                    raise ValueError(
                        f"Multiple WHERE clauses for {col} are not allowed, got {len(clauses)} clauses.")
                for clause in clauses:
                    if col_type == "boolean":
                        if clause.operator not in ["=", "<>", "IS", "IS NOT"]:
                            raise ValueError(
                                f"WHERE clauses for boolean column {col} can only use operators '=', '<>', 'IS', or 'IS NOT', got {clause.operator}")
                    else:
                        if clause.operator not in ["="]:
                            raise ValueError(
                                f"WHERE clauses for non-boolean column {col} can only use operators '=', got {clause.operator}")

    def _get_query(self,
                   quals: List[Qual],
                   columns: Set[str],
                   ) -> List[dict]:
        """
        Construct the query to execute against ApertureDB.
        This is used to build the query based on the columns and options.
        """
        listable_columns = {
            col for col in columns if self._columns[col].listable}

        modifying_columns = {
            col for col in columns if self._columns[col].modify_command_body is not None}

        command_body = {}

        # Apply table modification, e.g. with_class, set
        if self._options.modify_command_body:
            self._options.modify_command_body(command_body=command_body)

        # apply constraints
        self._apply_constraints(quals, columns, command_body=command_body)

        # Apply column modifications
        for qual in quals:
            if qual.field_name in modifying_columns:

                n_clauses = len(
                    [qual2 for qual2 in quals if qual2.field_name == qual.field_name])
                if n_clauses > 1:
                    raise ValueError(
                        f"Multiple WHERE clauses for {qual.field_name} are not allowed, got {n_clauses} clauses.")

                value = self._convert_qual_value(qual)
                column_options = self._columns[qual.field_name]
                column_options.modify_command_body(
                    command_body=command_body, value=value)

        # Check whether anyone has added the blobs parameter
        blobs = command_body.get("blobs", False)
        batch_size = BATCH_SIZE_WITH_BLOBS if blobs else BATCH_SIZE
        command_body["batch"] = {
            "batch_id": 0,
            "batch_size": batch_size
        }

        # In SQL it is legal to query a table with no result columns.
        # This will (by design) cause an error in ApertureDB.
        if not blobs and not listable_columns:
            listable_columns = {"_uniqueid"}

        if listable_columns:
            command_body["results"] = {"list": list(listable_columns)}

        query = [{self._options.command: command_body}]
        logger.debug(
            f"Constructed query: {compact_pretty_json(query, indent=2)}")

        return query

    def _apply_constraints(self, quals: List[Qual], columns: Set[str], command_body: Dict[str, Any]) -> None:
        """
        Apply constraints to the command body based on the provided qualifications and columns.
        """
        logger.debug(
            f"Applying constraints for quals: {quals} and columns: {columns}")
        listable_columns = {
            col for col in columns if self._columns[col].listable}
        for qual in quals:
            if qual.field_name in listable_columns:
                constraint = self._apply_constraint(qual)
                if constraint is not None:
                    logger.debug(
                        f"Applying constraint {constraint} for qual {qual} on column {qual.field_name}")
                    if "constraints" not in command_body:
                        command_body["constraints"] = {}
                    if qual.field_name not in command_body["constraints"]:
                        command_body["constraints"][qual.field_name] = []
                    command_body["constraints"][qual.field_name].extend(
                        constraint)

    def _apply_constraint(self, qual: Qual) -> Optional[List[Any]]:
        """
        Apply constraint to the command body based on the provided qualification.
        """
        col_type = self._columns[qual.field_name].type
        value = self._convert_qual_value(qual, use_operator=False)
        if col_type == "number":
            assert value is None or isinstance(
                value, (int, float)) or (isinstance(value, list) and all(isinstance(x, (int, float)) for x in value)), f"Qual {qual} for column {qual.field_name} must be a number for operator {qual.operator}, found {value} is {type(value)} (was {type(qual.value)})."
        logger.debug(
            f"Applying constraint for qual {qual} with field name {qual.field_name}, operator {qual.operator}, value {value} and column type {col_type}")
        if qual.operator == "=":
            if isinstance(value, list):
                return ["in", value]
            else:
                return ["==", value]
        elif qual.operator == "<>":
            # SQL NULL semantics will not include results where a column is NULL, but ApertureDB will.
            if value is None:
                # This is a special case for NULL values; we want to include rows where the column is NULL.
                return ["!=", None]
            elif isinstance(value, list):
                return ["not in", value]
            elif col_type == 'boolean':
                return ["==", not value]
            else:
                return ["not in", [value, None]]
        elif qual.operator == "IS":
            assert col_type == 'boolean' or value is None, f"Qual {qual} for column {qual.field_name} must be boolean or None for IS operator."
            return ["==", value]
        elif qual.operator == "IS NOT":
            assert col_type == 'boolean' or value is None, f"Qual {qual} for column {qual.field_name} must be boolean or None for IS NOT operator."
            # This will include null values (if value is not None)
            return ["!=", value]
        elif qual.operator == "IN" or qual.operator == ("=", True):
            assert isinstance(
                value, list), f"Qual {qual} for column {qual.field_name} must be a list for IN operator."
            if None in value:  # SQL NULL handling rules
                value.remove(None)
            return ["in", value]
        elif qual.operator == "NOT IN" or qual.operator == ("<>", False):
            assert isinstance(
                value, list), f"Qual {qual} for column {qual.field_name} must be a list for NOT IN operator."
            if None in value:
                return ["in", []]  # exclude all rows; SQL NULL handling rules
            else:
                return ["not in", value, "!=", None]
        elif col_type == "boolean":
            # Standard SQL does not support < or > for boolean columns, but this is a PostgreSQL-specific extension.
            # FALSE < TRUE, and comparison with NULL is always UNKNOWN which is treated as FALSE.
            if qual.operator == "<":
                if value is True:
                    return ["==", False]
                elif value is False:
                    return ["in", []]  # surprisingly this is allowed
            elif qual.operator == "<=":
                if value is False:
                    return ["==", False]
                elif value is True:
                    return ["!=", None]
            elif qual.operator == ">":
                if value is False:
                    return ["==", True]
                elif value is True:
                    return ["in", []]  # surprisingly this is allowed
            elif qual.operator == ">=":
                if value is True:
                    return ["==", True]
                elif value is False:
                    return ["in", [False, True]]
            # everything else is a no-op for boolean columns
        elif col_type in {"datetime", "number", "string"} and qual.operator in {"<", "<=", ">", ">="}:
            return [qual.operator, value]
        # Skip this qual; PostgreSQL will filter it out later if it's important.
        logger.debug(
            "Ignoring qual {qual}, col_type {col_type}, value {value}")
        return None

    def _convert_qual_value(self, qual: Qual, use_operator=True) -> Any:
        """
        Convert qual value into an internal value, depending on type and operator.
        """
        if isinstance(qual.value, (list, tuple)):
            return [self._convert_qual_value(Qual(qual.field_name, qual.operator, x), use_operator=use_operator) for x in qual.value]

        col_type = self._columns[qual.field_name].type
        if col_type == "boolean":
            value = True if qual.value == 't' else False if qual.value == 'f' else None
            if qual.operator in ["<>", "IS NOT"] and use_operator and value in [True, False]:
                value = not value
        elif col_type == "json":
            try:
                value = json.loads(qual.value)
            except json.JSONDecodeError as e:
                raise ValueError(
                    f"Invalid JSON in {qual.field_name} clause: {e}")
        elif col_type == "datetime":
            # convert datetime datetime objects to the internal format
            if isinstance(qual.value, datetime):
                value = {"_date": qual.value.isoformat()}
            else:
                raise ValueError(
                    f"Invalid datetime value for {qual.field_name}: {qual.value}. Expected a datetime object.")
        elif col_type == "number":
            value = float(qual.value) if qual.value is not None else None
        else:
            # TODO: Might be more conversions needed here
            value = qual.value

        return value

    def _get_query_blobs(self, quals: List[Qual], columns: Set[str]) -> List[bytes]:
        query_blob_quals = [qual for qual in quals
                            if self._columns[qual.field_name].query_blobs is not None]

        if query_blob_quals:
            if len(query_blob_quals) > 1:
                raise ValueError(
                    f"Multiple query blobs requested: {query_blob_quals}. Only one query blob is allowed per query.")

            qual = query_blob_quals[0]
            value = self._convert_qual_value(qual)
            return self._columns[qual.field_name].query_blobs(value=value)

        return []

    def _get_query_results(self,
                           query: List[dict],
                           query_blobs: List[bytes],
                           ) -> Generator[Tuple[dict, Optional[bytes]], None, List[dict]]:
        logger.debug(f"Executing query: {query}")

        start_time = datetime.now()
        _, results, response_blobs = get_pool().execute_query(query, query_blobs)
        elapsed_time = datetime.now() - start_time
        logger.info(
            f"Query executed in {elapsed_time.total_seconds()} seconds. Results: {results}, Blobs: {len(response_blobs) if response_blobs else 0}")

        if not results or len(results) != 1:
            logger.warning(
                f"No results found for entity query. {query} -> {results}")
            raise ValueError(
                f"No results found for entity query: {query}. Please check the class and columns. Results: {results}")

        result_objects = results[0].get(
            self._options.command, {}).get(self._options.result_field, [])
        if not response_blobs:
            for row in result_objects:
                yield row, None
        else:
            for row, blob in zip_longest(result_objects, response_blobs):
                yield row or {}, blob

        return results, elapsed_time

    def _post_process_row(self,
                          quals: List[Qual], columns: Set[str],
                          row: dict, blob: Optional[bytes]) -> dict:
        """
        Apply post-processing steps to the row before yielding it.

        This includes converting types from AQL to SQL,
        adding non-list columns, and any column post-processing.
        """
        row = row.copy()  # Avoid modifying the original row

        for qual in quals:
            if self._columns[qual.field_name].post_process_results is not None:
                value = self._convert_qual_value(qual)
                self._columns[qual.field_name].post_process_results(
                    row=row, value=value, blob=blob)

        # Normalize the row to ensure it has the correct types for PostgreSQL
        row = self._normalize_row(columns, row)

        row = self._add_non_list_columns(quals, columns, row)

        return row

    def _normalize_row(self, columns, row: dict) -> dict:
        """
        Normalize a row to ensure it has the correct types for PostgreSQL.
        This is used to convert ApertureDB types to PostgreSQL types.
        """
        result = {}
        for col in columns:
            if col not in row:
                continue
            value = self._convert_adb_value(row[col], col)
            result[col] = value
        return result

    def _convert_adb_value(self, value, col: str) -> Any:
        """
        Convert an ApertureDB value to a PostgreSQL-compatible value.
        """
        col_type = self._columns[col].type
        if col_type == "datetime":
            value = value["_date"] if value else None
        elif col_type == "json":
            value = json.dumps(value)
        elif col_type == "blob":
            value = value
        # elif col_type == "boolean":
        #     value = 't' if value else 'f'
        else:
            value = value
        return value

    def _add_non_list_columns(self, quals: List[Qual], columns: Set[str],  row: dict) -> dict:
        """
        Add non-list columns to the row based on the quals.
        This is used to ensure that all requested columns are present in the result.

        This is necessary because PostgreSQL will not return rows that don't meet the quals, and it doesn't know that we're
        using special columns to do magic.

        So we just copy the qual constraints into the row.
        """
        logger.debug(
            f"Adding non-list columns to row: {row}, quals: {quals}, columns: {columns}")
        row = row.copy()  # Avoid modifying the original row
        for qual in quals:
            if not self._columns[qual.field_name].listable:
                assert qual.field_name not in row, f"Column {qual.field_name} should not be in the row. It is a non-list column."
                logger.debug(
                    f"Adding non-list column {qual.field_name} with value {qual.value} to row"
                )
                # This double-conversion is necessary because of negative qual operators like IS NOT and <>.
                value = self._convert_qual_value(qual)
                value = self._convert_adb_value(value, qual.field_name)
                row[qual.field_name] = value

        return row

    def _get_next_query(self, query: List[dict], response: List[dict]) -> Optional[List[dict]]:
        """
        Get the next query to execute based on the response from the previous query.
        This is used to handle batching.
        """
        if not response or len(response) != 1:
            logger.warning(
                f"No results found for query: {query} -> {response}")
            return None

        if "batch" not in response[0][self._options.command]:
            # Some commands (like FindConnection) don't handle batching, so we assume all results are returned at once.
            logger.info(
                f"Single batch found for query: {query} -> {response[:10]}")
            return None

        batch_id = response[0][self._options.command]["batch"]["batch_id"]
        total_elements = response[0][self._options.command]["batch"]["total_elements"]
        end = response[0][self._options.command]["batch"]["end"]

        if end >= total_elements:  # No more batches to process
            return None

        next_query = query.copy()
        next_query[0][self._options.command]["batch"]["batch_id"] += 1
        return next_query

    def explain(self, quals: List[Qual], columns: Set[str], sortkeys=None, verbose=False) -> Iterable[str]:
        """
        Generate an EXPLAIN statement for the FDW query.
        This is used to provide information about how the query will be executed.
        """
        logger.info(
            f"Explaining FDW {self._options.table_name} with quals: {quals} and columns: {columns}")
        self._check_quals(quals, columns)

        result = {}

        result["table_name"] = self._options.table_name
        result["quals"] = [[qual.field_name, qual.operator, qual.value]
                           for qual in quals]
        result["columns"] = list(columns)
        result["aql"] = self._get_query(quals, columns)
        # This part isn't verbose, but can be much slower
        if verbose:
            query_blobs = self._get_query_blobs(quals, columns)
            if query_blobs:
                result["query_blob_lengths"] = [
                    len(blob) for blob in query_blobs]

        return [compact_pretty_json(result)]


logger.info("FDW class defined successfully")
