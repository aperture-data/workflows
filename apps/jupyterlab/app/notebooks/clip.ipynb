{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP Embeddings and NLP prompt\n",
    "\n",
    "If you run the [Generate Embeddings Workflow](https://docs.aperturedata.io/workflows/embeddings_extraction), \n",
    "embeddings will be computed for each image and PDF present on ApertureDB, using the \"ViT-B/16\" model from [OpenAI CLIP](https://openai.com/index/clip/).\n",
    "\n",
    "We can query these using a natural language prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aperturedb.CommonLibrary import create_connector\n",
    "from aperturedb import NotebookHelpers as nh\n",
    "\n",
    "# This will only work if you have apererturedb installed and configured.\n",
    "# The configuration is either created by setting an APERTUREDB_KEY environment variable,\n",
    "# or by creating a configuration using adb config.\n",
    "# See https://docs.aperturedata.io/Setup/client/adb for more information.\n",
    "client = create_connector(key=\"Get this key from the ApertureDB instance\")\n",
    "\n",
    "# If you wish to explicitly use the Connector class, you can do so like this:\n",
    "#from aperturedb import Connector as Connector\n",
    "#client = Connector.Connector(host=\"<DB_HOST>\", user=\"admin\", password=\"<YOUR_PASSWORD_HERE>\")\n",
    "\n",
    "response, _ = client.query([{\"GetStatus\": {}}])\n",
    "client.print_last_response()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find images similar to text\n",
    "\n",
    "Assuming that we have imported some images,\n",
    "we can search for them using a text caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=\"cpu\")\n",
    "\n",
    "# Natural language prompt\n",
    "prompt = \"a photo of a group of more than 2 people\"\n",
    "# prompt = \"a photo of people with dogs\"\n",
    "# prompt = \"a photo of a happy baby\"\n",
    "\n",
    "search_tokens = clip.tokenize([prompt]).to(\"cpu\")\n",
    "search_embeddings = model.encode_text(search_tokens)\n",
    "\n",
    "blobs = search_embeddings[0].detach().numpy().tobytes()\n",
    "\n",
    "query = [{\n",
    "    \"FindDescriptor\": {\n",
    "        \"_ref\": 1,\n",
    "        \"k_neighbors\": 10,\n",
    "        \"set\": \"wf_embeddings_clip\",\n",
    "    }\n",
    "}, {\n",
    "    \"FindImage\": {\n",
    "        \"_ref\": 2,\n",
    "        \"blobs\": True,\n",
    "        \"is_connected_to\": {\n",
    "            \"ref\": 1\n",
    "        },\n",
    "        \"results\": {\n",
    "            \"limit\": 10\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "r, blobs = client.query(query, [blobs])\n",
    "client.print_last_response()\n",
    "\n",
    "print(len(blobs))\n",
    "nh.display(blobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find text similar to text\n",
    "\n",
    "This assumes that you have text segments, perhaps ingested from PDFs, and used something like the \"Generate Embeddings\" workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aperturedb import NotebookHelpers as nh\n",
    "import clip\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "# Natural language prompt\n",
    "prompt = \"insert descriptive phrase here\"\n",
    "\n",
    "search_tokens = clip.tokenize([prompt]).to(\"cpu\")\n",
    "search_embeddings = model.encode_text(search_tokens)\n",
    "descriptor_set = \"wf_embeddings_clip_text\"\n",
    "\n",
    "blobs = search_embeddings[0].detach().numpy().tobytes()\n",
    "\n",
    "query = [{\n",
    "    \"FindDescriptor\": {\n",
    "        \"k_neighbors\": 10,\n",
    "        \"set\": descriptor_set,\n",
    "        \"results\": {\"all_properties\": True},\n",
    "    }\n",
    "}]\n",
    "\n",
    "r, _ = client.query(query, [blobs])\n",
    "# client.print_last_response()\n",
    "entities = r[0][\"FindDescriptor\"][\"entities\"]\n",
    "for e in entities:\n",
    "    print(f\"{e.get('title')} page {e.get('page_number')}\\n{e.get('text')}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Parts in video most closely resembling the input prompt.\n",
    "\n",
    "This assumes that the instance has atleast 1 video already ingested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aperturedb import NotebookHelpers as nh\n",
    "import clip\n",
    "from aperturedb.CommonLibrary import execute_query\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "# Natural language prompt\n",
    "prompt = \"Descriptive prompt here\"\n",
    "\n",
    "search_tokens = clip.tokenize([prompt]).to(\"cpu\")\n",
    "search_embeddings = model.encode_text(search_tokens)\n",
    "descriptor_set = \"wf_embeddings_clip_video\"\n",
    "\n",
    "blobs = search_embeddings[0].detach().numpy().tobytes()\n",
    "\n",
    "query = [{\n",
    "    \"FindDescriptor\": {\n",
    "        \"k_neighbors\": 10,\n",
    "        \"set\": descriptor_set,\n",
    "        \"results\": {\"all_properties\": True},\n",
    "        \"_ref\": 1\n",
    "    }\n",
    "}, {\n",
    "    \"FindClip\": {\n",
    "        \"_ref\": 2,\n",
    "        \"is_connected_to\": {\n",
    "            \"ref\": 1\n",
    "        },\n",
    "        \"results\": {\n",
    "            \"group_by_source\": True,\n",
    "            \"all_properties\": True\n",
    "        },\n",
    "    }\n",
    "},{\n",
    "    \"FindVideo\": {\n",
    "        \"is_connected_to\": {\n",
    "            \"ref\": 2\n",
    "        },\n",
    "        \"results\": {\n",
    "            \"all_properties\": True\n",
    "        }\n",
    "    }\n",
    "}\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "status, r, cblobs = execute_query(client, query, [blobs])\n",
    "assert status == 0, r\n",
    "\n",
    "descriptors = r[0][\"FindDescriptor\"][\"entities\"] # This is a list of descriptors\n",
    "clips = r[1][\"FindClip\"][\"entities\"] # This is a dictionary of source -> list of clips\n",
    "videos = r[2][\"FindVideo\"][\"entities\"]\n",
    "print(f\"{len(clips)=} {len(videos)=}\")\n",
    "\n",
    "# Get a mapping of descriptor id to clip id\n",
    "clip_ids = {d_id: c[0].get('_uniqueid') for d_id, c in clips.items()}\n",
    "video_ids = [v.get('_uniqueid') for v in videos]\n",
    "\n",
    "\n",
    "# Show the clips for the first 5 descriptors\n",
    "for d in descriptors[0:5]:\n",
    "    d_id = d.get('_uniqueid')\n",
    "    clip_id = clip_ids[d_id]\n",
    "    q = [{\n",
    "        \"FindClip\": {\n",
    "            \"constraints\": {\n",
    "                \"_uniqueid\": [\"==\", clip_id]\n",
    "            },\n",
    "            \"results\": {\n",
    "                \"all_properties\": True\n",
    "            },\n",
    "            \"blobs\": True\n",
    "        }\n",
    "    }]\n",
    "    status, r, cblobs = execute_query(client, q, [])\n",
    "    assert status == 0, r\n",
    "    nh.display_video_mp4(cblobs[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the full videos the clips are derived from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the clips for the first 5 descriptors\n",
    "retrieved_videos = set()\n",
    "for v_id in video_ids[0:5]:\n",
    "    q = [{\n",
    "        \"FindVideo\": {\n",
    "            \"constraints\": {\n",
    "                \"_uniqueid\": [\"==\", v_id]\n",
    "            },\n",
    "            \"results\": {\n",
    "                \"all_properties\": True\n",
    "            },\n",
    "            \"blobs\": True\n",
    "        }\n",
    "    }]\n",
    "\n",
    "    if v_id not in retrieved_videos:\n",
    "        status, r, vblobs = execute_query(client, q, [])\n",
    "        assert status == 0, r\n",
    "        nh.display_video_mp4(vblobs[0])\n",
    "        retrieved_videos.add(v_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "package",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
